{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe451aa-f388-408b-ab10-5039905173cb",
   "metadata": {},
   "source": [
    "Understanding Components of a Custom DataLoader in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d63217-dfe4-4005-b802-e899de2c22e6",
   "metadata": {},
   "source": [
    "1. Dataset `torch.utils.data.Dataset`\n",
    "2. DataLoader `torch.utils.data.DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ebbc3b-e01b-41d8-811c-cfda22e86eaf",
   "metadata": {},
   "source": [
    "Creating custom dataset inPytorch\n",
    "\n",
    "- `init()` - initialised the dataset , loads data , applied preprocessing\n",
    "- `len()`  - return the total numbers odf samples in the dataset \n",
    "- `getitem()` - Defines how to review a single data sample when index is provide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf59662-9d3c-48a7-8b90-75843999f1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d7b031a-4b42-4b4f-9d94-4d0162e10712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373373f6-828f-4d81-abd8-4af1a7ac11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset , DataLoader\n",
    "import os\n",
    "from PIL import Image \n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset , DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1eea4d-4ad7-4c9c-9598-b8652172e15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cat\n",
      "1 Dog\n",
      "2 person\n"
     ]
    }
   ],
   "source": [
    "image_dir = r\"C:\\Users\\hp\\Downloads\\Classification_dataset_v3\\Classification_dataset_v3\\images\\train\"\n",
    "for label , class_dir in enumerate(os.listdir(image_dir)):\n",
    "    print(label , class_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd848c-49e6-4b08-831e-cb8cf229f2d9",
   "metadata": {},
   "source": [
    "- `self.class_name` - Stores the mapping between label and class name.\n",
    "- `class_path` - Holds the path to one class folder at a time\n",
    "-  `self.image_paths` - Stores full paths to all images across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebbf1e5-6f08-47fc-a156-9167d2f26f02",
   "metadata": {},
   "source": [
    "so the class_names has label and its corresponding class and class_path has the images of one class and image_paths has the path to that images with its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab29bf07-a7e5-4159-8b9b-71dc1086081a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self,image_dir,transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.class_name = {}\n",
    "        self.transform = transform\n",
    "\n",
    "        for label , class_dir in enumerate(os.listdir(image_dir)):\n",
    "            self.class_name[label] = class_dir\n",
    "            class_path = os.path.join(image_dir , class_dir)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                self.image_paths.append(os.path.join(class_path , img_name))\n",
    "                self.labels.append(label)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self , idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image , label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "180ddf1a-67fc-4fd1-9701-0fcaac843dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128 , 128)),\n",
    "    transforms.ToTensor() ,\n",
    "    transforms.Normalize(mean=[0.5 , 0.5 , 0.5] , std = [0.5 , 0.5 , 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43bfe99a-3fd0-4b59-abb6-0668078cb398",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_dir = r\"C:\\Users\\hp\\Downloads\\Classification_dataset_v3\\Classification_dataset_v3\\images\\train\"\n",
    "test_image_dir = r\"C:\\Users\\hp\\Downloads\\Classification_dataset_v3\\Classification_dataset_v3\\images\\test\"\n",
    "\n",
    "train_image_dataset = ImageDataset(image_dir = train_image_dir , transform=transform)\n",
    "test_image_dataset = ImageDataset(image_dir = test_image_dir , transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87e31ed7-4e24-474e-b7f3-4c61fb78f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_loader = DataLoader(dataset = train_image_dataset , batch_size=32 , shuffle=True)\n",
    "test_image_loader = DataLoader(dataset = test_image_dataset , batch_size=32 , shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b499ab19-bc1a-4153-8e8c-d4ed0943799a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for images , labels in train_image_loader:\n",
    "    print(images.shape , labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa71a941-b40b-452e-819d-6d92e4fc3ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Cat', 1: 'Dog', 2: 'person'}\n",
      "{0: 'Cat', 1: 'Dog', 2: 'person'}\n"
     ]
    }
   ],
   "source": [
    "print(train_image_dataset.class_name)\n",
    "print(test_image_dataset.class_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e82ae60-e0ba-4241-9c93-e3291107e735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 128, 128]) torch.Size([32])\n",
      "person\n",
      "(3, 128, 128)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for images , labels in train_image_loader:\n",
    "    print(images.shape , labels.shape)\n",
    "    img = images[0].numpy()\n",
    "    label=labels[0].item()\n",
    "    print(train_image_dataset.class_name[label])\n",
    "    np.transpose(img , (1,2,0))\n",
    "    print(img.shape)\n",
    "    print(label)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15190260-ea88-461d-9009-8585ef4b46f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e54e20-8fc2-470d-a586-eeb16203a575",
   "metadata": {},
   "source": [
    "so the 3 is the channels the 3x3 is the filter map and 32 are the no of the features we want is that right and in output we get 32 features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39bfbdbd-b6aa-4b88-ac43-c830b478e190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCnnModule(nn.Module):\n",
    "    def __init__(self,input_dim , num_classes):\n",
    "        super(CustomCnnModule , self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        #32 is the number of feature maps produced by the convolution.\n",
    "        #kernel_size - The size of each convolution filter\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Fully Connected Layer 1\n",
    "            nn.Conv2d(3,32,kernel_size=3,stride=1 , padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2 , stride=2),\n",
    "\n",
    "            # Fully Connected layer 2\n",
    "            nn.Conv2d(32 , 64 , kernel_size=3 , stride=1 , padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2 , stride=2),\n",
    "\n",
    "            # Fully Connected Layer 3\n",
    "            nn.Conv2d(64 , 128 , kernel_size=3 , stride=1 , padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2 , stride=2),\n",
    "\n",
    "            # Fully Connected layer 4\n",
    "            nn.Conv2d(128 , 256, kernel_size=3 , stride=1 , padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2 , stride=2),\n",
    "        )\n",
    "\n",
    "        self._to_linear = None\n",
    "        self._get_conv_output(self.input_dim)\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(self._to_linear , 512),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.2)\n",
    "            nn.Linear(512 , 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128 , self.num_classes)\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    def _get_conv_output(self , input_dim=128):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1,3,input_dim ,input_dim)\n",
    "            output = self.conv_layers(dummy_input)\n",
    "            self._to_linear = output.view(1 , -1).size(1)\n",
    "            \n",
    "    def forward(self , x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0) , -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c52c68b-e25e-49c0-bb0f-3df16de6ed64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomCnnModule(input_dim = 128 , num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f323da01-546b-40d0-8fa8-e0405ac36b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomCnnModule(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=16384, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=3, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bca0732-b999-4033-8985-53ce48b90bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters() , lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6358172a-d333-46bd-8af4-0066b6247cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 , Loss: 0.9256369948387146\n",
      "Epoch 2/10 , Loss: 0.6286568368736066\n",
      "Epoch 3/10 , Loss: 0.5373990131051917\n",
      "Epoch 4/10 , Loss: 0.4958054642928274\n",
      "Epoch 5/10 , Loss: 0.43070179365183175\n",
      "Epoch 6/10 , Loss: 0.41985822759176555\n",
      "Epoch 7/10 , Loss: 0.3756843610813743\n",
      "Epoch 8/10 , Loss: 0.32743206322193147\n",
      "Epoch 9/10 , Loss: 0.30248469333899647\n",
      "Epoch 10/10 , Loss: 0.29168898486777356\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images , labels in train_image_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs , labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs} , Loss: {running_loss/len(train_image_loader)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8aedf1cd-1ae3-4388-bf1c-d7a8a58fd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict() , \"CNN_Model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91dcff4a-469c-4620-a731-489bcdcb1185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is : 79.50%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_image_loader:\n",
    "        outputs = model(images)\n",
    "\n",
    "        _, predicted = torch.max(outputs , 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Test Accuracy is : {100* correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "85837cbb-8f73-4769-b6c3-80794fd2b4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class id : Dog\n"
     ]
    }
   ],
   "source": [
    "classifier = ImageClassifier('Documents/PyTorch/CNN_Model.pth', train_image_dataset.class_name)\n",
    "label = classifier.predict(r\"C:\\Users\\hp\\Downloads\\images (1).webp\")\n",
    "print(f\"Predicted Class id : {label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
